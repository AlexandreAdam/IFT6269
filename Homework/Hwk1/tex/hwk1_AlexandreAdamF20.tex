\documentclass{article}
\usepackage[a4paper, margin=2cm]{geometry}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{amssymb}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}

\pagestyle{fancy}
\rhead{Alexandre Adam \\ 20090755}
\lhead{IFT6269 â€• Probabilistic Graphical Models \\ Simone Lacoste-Julien}
\chead{Homework 1}
\rfoot{\today}
\cfoot{\thepage}


\newcommand{\s}{\hspace{0.1cm}}
\numberwithin{equation}{section}
\renewcommand\thesubsection{\alph{subsection})}
\renewcommand\thesubsubsection{\Roman{subsubsection}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\mle}[1]{\hat{#1}_{\text{MLE}}}
\newcommand{\set}[1]{\{#1\}}

\begin{document}

\section{Probability and independence}
\subsection{Decomposition}
We aim to validate
\begin{equation}
        (X \perp Y, W \mid  Z)  \implies (X \perp Y \mid Z)
\end{equation}
\textit{Proof}.  We suppose the statement $(X \perp Y,\ W \mid Z)$ is true. 
It follows from the definition of the conditional independence that
$p(x, y, w \mid z) = p(x\mid z) p(y, w\mid z)$ for all 
$x \in  \Omega_x$, $(y, w) \in \Omega_y \times  \Omega_w$  and $z \in  \Omega_z$.
We then consider the marginalize  $p(x,y,w \mid z)$:
\begin{align*}
        p(x,y|z) &=  \sum_{w\s \in \s \Omega_w}  p(x, y , w \mid z) \\
                 &=  \sum_{w\s \in \s \Omega_w} p(x\mid z)p(y,w \mid z) \\
                 &= p(x \mid z) \sum_{w\s  \in \s  \Omega_w} p(y,w \mid z) \\
                &=  p(x\mid z) p(y\mid  z)
\end{align*}
from which we conclude that  $(X \perp Y \mid Z) \qed$.
By symmetry of the argument,  we can show that ${(X \perp W \mid Z)}$ is true as well.

\subsection{}
We aim to validate 
\begin{equation}\label{eq:1b}
       (X \perp Y \mid Z) \s \text{and} \s (X, Y \perp  W \mid Z) \implies (X \perp W \mid Z)  
\end{equation}
\textit{Proof}. Suppose $(X, Y \perp  W \mid Z)$ and $(X \perp Y \mid Z)$ are true.
We know from the symmetry and decomposition properties of  
the  conditional independence that  
$(X, Y \perp W \mid  Z)  \implies  (W \perp X, Y \mid Z)  \implies  (X \perp  W \mid Z)$.
Therefore $ (X \perp W \mid Z)$ is true $\qed$.


\subsection{}
We aim to validate
\begin{equation}
        (X \perp Y,W \mid Z) \s \text{and} \s (Y \perp W \mid Z) \implies  (X,W \perp Y \mid Z)
\end{equation}
\textit{Proof}. Suppose $(X \perp Y,W \mid Z)$ is true. 
Then it follows from the definition of conditional independence that
\begin{align*}
        p(x,y,w \mid z) = p(x \mid z) p(y,w \mid z) 
\end{align*}
Then assume $(Y \perp W \mid Z)$ is true. The second factor can be factorized
\begin{align*}
        p(x,y,w \mid z) = p(x \mid z) p(y \mid z) p(w \mid z)
\end{align*}
From the decomposition property, we know $(X \perp W \mid Z)$ is true. Thus
\begin{align*}
        p(x,y,w \mid z)  = p(x,w \mid z) p(y \mid z)
\end{align*}
From which we conclude $(X,W \perp Y \mid Z)$ is true $\qed$.

\subsection{}
We aim to validate
\begin{equation}
        (X \perp Y \mid Z) \s \text{and} \s (X \perp Y \mid W) \implies (X \perp Y \mid Z,W)
\end{equation}
\textit{Counter example}. We consider the following R.V.
\begin{enumerate}
        \item X: Person A arrive late for diner;
        \item Y: Person B arrive late for diner;
        \item W: They come from the same city;
        \item Z: They work in the same city.
\end{enumerate}
For this situation, we see that X and Y are conditionally independent when 
given either W or Z. If we know they are from the same city, then they might work 
in different cities and take different route home. Thus knowing person A was late doesn't inform 
us on the probability of person B to arrive late. \par
A similar argument can be made for  $(X \perp Y \mid Z)$. \par
Thus the LHS of the proposition is true, yet the RHS is clearly false in our case. 
Assuming we were given that W and Z are true, then we are given the geolocalisation 
of person A and B.
If we were given that 
person A would be late for diner, then we'd be able to make a good guess that person B would 
be late as well (they would both be impacted by the same traffic jam or whatnot).
Thus the proposition is false.

\section{Bayesian inferance and MAP}
Let $\mathbf{X}_1, \dots , \mathbf{X}_n \bm{\mid} \bm{\pi} \overset{\text{iid} }{\sim} 
\text{Multinomial} (1, \bm{ \pi} )$ on k element. The values are sampled from a set of cardinality 
2, that is $x^{(i)}_j \in \left\{ 0, 1 \right\}$. Each R.V. has only one non-zero 
entry for a given trial, that is $\sum_{j = 1}^k x^{(i)}_j = 1$. \par
We assume a Dirichlet prior $\bm{ \pi} \sim \text{Dir} ( \bm{ \alpha} )$ with 
a PDF
\[
        p(\bm{ \pi}  \mid \bm{ \alpha} ) = \frac{\Gamma ( \sum _{i = 1}^k \alpha_j) }{
        \prod_{j=1}^k \Gamma(\alpha_j)} \prod_{j = 1}^k \pi_j^{\alpha_j - 1}
\]

\subsection{}
Since the data is IID, they are mutually independent by definition. Being given the 
parameters of their Multinomial distribution (or a subset for that matter) 
does not change the independence of the $\mathbf{X}$'s. Thus, 
\[
        (\mathbf{X}_i \perp \mathbf{X}_j \mid \bm{ \pi}) \s \forall \s (i,j) \s \in 
        \left\{ 1,\dots ,k \right\} \times \left\{ 1,\dots ,k \right\}
\]
Of course, none of the vector can be mutually nor conditionally independent to $\bm{ \pi} $ 
since it contains information about the distribution of the one hot vectors $\mathbf{X}_i$. 
In this case $\bm{ \pi} $ are the probabilities of one of the $k$ entry to be equal 
to one. Even giving one of these away is enough to impact the posterior distribution 
of the conditional $p(x_i\mid x_\ell, \pi_j )$ for example.

\subsection{}
The posterior distribution $p( \bm{ \pi} \mid x_1 , \dots , x_n) $ is computed via 
the Bayes rule
\[
        p(\bm{ \pi} \mid \mathbf{x}_{1:n}) = 
        \frac{p(\mathbf{x}_{1:n} \mid \bm{ \pi} ) p(\bm{ \pi}) }{p(\mathbf{x}_{1:n})}
\]
where $p(\bm{ \pi} ) = p(\bm{ \pi} \mid \bm{\alpha} )$ is the prior for $\bm{ \pi} $
defined above. For the sake of determining the 
posterior distribution, we can postpone the derivation of the marginal likelihood.
%The likelihood is the joint probability of $n$ 
%The likelihood $p(\mathbf{x}_{1:n} \mid \bm{ \pi} )$ is the probability mass function 
%corresponding to $n$ trials of a $k-$sided die throw. We define the vector 
%$\bm{\chi} \equiv \sum_{i=1}^n \mathbf{x}_j$ with the property
%\[
        %\sum_{j = 1}^k \chi_j = n
%\]
%It becomes clear that the likelihood follows the 
%$ \text{Multinomial}(n, \bm{ \pi})  $ distribution. 
%The PMF is given by
%\[
        %p(\mathbf{x}_{1:n} \mid \bm{ \pi} ) =
%\binom{n}{\chi_1,\dots,\chi_k}
        %\prod_{j=1}^k \pi_j^{\chi_j}
        %\propto 
        %\prod_{i = 1}^n
        %\prod_{j=1}^k \pi_j^{x_j^{(i)}}
%\]
%Where it is agreed that $\chi_j = \sum_{i = 1}^n x^{(i)}_j$. 
Therefore, the posterior must be
\[
        p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) \propto 
        \prod_{i = 1}^n \prod_{j=1}^k \pi_j^{x_j^{(i)}}
        %\left( \frac{\Gamma(\sum_{\ell = 1}^k \alpha_\ell)}{\prod_{\ell}^k \Gamma(\alpha_{\ell})}
                \prod_{\ell = 1}^k \pi_\ell^{\alpha_\ell -1}
        %\right) 
\] 
We use the fact that we can swap around product operator for real numbers.
\[
         p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) \propto 
        \prod_{i = 1}^n \prod_{j=1}^k 
                      \prod_{\ell = 1}^k \pi_\ell^{\alpha_\ell -1}\pi_j^{x_j^{(i)}}
                      = \prod_{j = 1}^k \pi_j^{\sum_{i = 1}^n x_j^{(i)} + \alpha_j -1}

\]
We can readily see that the resulting distribution will be a Dirichlet with 
updated $\alpha_\ell$'s. 

\textbf{The posterior distribution is a \underline{Dirichlet} distribution with parameters 
$\alpha_j'= \alpha_j + \sum_{i = 1}^n x_j^{(i)} $}.

\subsection{Marginal Likelihood}
 The marginal likelihood $p(\mathbf{x}_{1:n})$ 
is a normalizing constant defined as the 
integral of the numerator (in the Baye's rule) over all instantiation of $\bm{ \pi} $
\[
        p(\mathbf{x}_{1:n}) = \int_{\bm{ \Delta_k}} 
        p(\mathbf{x}_{1:n} \mid \bm{ \pi})  p(\bm{ \pi} )d^{(k)}\bm{ \pi}  
\]
where $\bm{ \Delta}_k $ is the probability simplex. In term of the quantities 
defined above, this is
\[
         p(\mathbf{x}_{1:n}) = \int_{\bm{ \Delta_k}} 
         d^{(k)}\bm{ \pi} 
         %\binom{n}{\chi_1,\dots,\chi_k}
         \prod_{j = 1}^k \pi_j^{\sum_{i =1}^n x_j^{(i)}}
        \left( 
         \frac{\Gamma ( \sum _{\ell = 1}^k \alpha_\ell) }{\prod_{\ell=1}^k \Gamma(\alpha_\ell)} 
         \prod_{\ell = 1}^k \pi_\ell^{\alpha_\ell - 1}
        \right) 
\]
The $\pi_j$'s are independent variables since the simplex $\bm{ \Delta}_k $ is crucially 
defined as an affine plane in an Euclidian space which is supported by 
a set of orthonormal vectors.
%The crucial point is that the base space is an Euclidian space 
%spaned by orthonormal vectors.
%Thus, our task is to evaluate $k$ identical integrals of the form
%\[
        %\int_0^1 d\pi_j \pi_j^{\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1}
        %%= 
        %%\left( \sum_{i = 1}^n x_j^{(i)} + \alpha_j \right)^{-1},
        %%\s\s\s
        %%\left\{ \alpha_j > 0 \right\}
%\]
To evaluate this, we use the fact that the marginalized conjugate prior must sum to 1
\[
        \frac{\Gamma ( \sum _{\ell = 1}^k \alpha_\ell) }{\prod_{\ell=1}^k \Gamma(\alpha_\ell)} 
        \int_{\Delta_k} d^{(k)}\bm{ \pi} \prod_{j =1}^k \pi_j^{\alpha_j - 1} = 1
\]
Thus, since both integral have the same form we assume
\[
\int_{\Delta_k} d^{(k)}\bm{ \pi}  \pi_j^{\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1}
        =
        \frac{\prod_{j = 1}^k \Gamma(\sum_{i = 1}^n x_j^{(i)} + \alpha_j)}{
        \Gamma\left(\sum_{j = 1}^k \left( \sum_{i = 1}^n x_j^{(i)} + \alpha_j \right) \right)}
\]
We then get
\[
        \boxed{p(\mathbf{x}_{1:n}) = 
         %\binom{n}{\chi_1,\dots,\chi_k}
         \frac{\Gamma ( \sum _{\ell = 1}^k \alpha_\ell) }{\prod_{\ell=1}^k \Gamma(\alpha_\ell)} 
 %\prod_{j = 1}^k\left( \sum_{i = 1}^n x_j^{(i)} + \alpha_j \right)^{-1}
        \frac{\prod_{j = 1}^k \Gamma(\sum_{i = 1}^n x_j^{(i)} + \alpha_j)}{
        \Gamma\left(\sum_{j = 1}^k \left( \sum_{i = 1}^n x_j^{(i)} + \alpha_j \right) \right)}
}
\]
We notice that the first factor will cancel the one coming from the numerator in the 
posterior, and the second factor is the updated normalization factor of the Dirichlet.
 \subsection{$\hat{\bm{\pi}}_{\text{MAP}}  $}
 The maximum \textit{a posteriori} of the Multinomial distribution can be written in 
 term of the log posterior 
 \[
         \bm{ \hat{\pi}}_{\text{MAP}} \equiv \underset{\bm{ \pi}  \s \in \s \bm{ \Delta}_k }{\text{argmax}}
         \s
         \log p(\bm{ \pi} \mid \mathbf{x}_{1:n}) 
 \]
 Where the probability simplex is defined as 
 \[
         \bm{\Delta }_k = \left\{ \bm{ \pi}  \in \mathbb{R}^k \Biggm| \pi_j \s\in [0,1]\s 
                 \text{ and }\s \sum_{j = 1}^k \pi_j = 1 
         \right\}
 \]
 We define the constraint as $g(\bm{ \pi})= 1 - \sum_{j=1}^k \pi_j $.
 We notice that 
 \[
         \log p(\bm{ \pi} \mid \mathbf{x}_{1:n}) = C + \sum_{j = 1}^k 
         \left( \sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1 \right)\log \pi_j 
 \]
where $C$ is the normalization constant. 
The optimisation of the log posterior becomes
 \begin{align*}
         \bm{ \hat{\pi}}_{\text{MAP}} &= 
         \underset{(\bm{ \pi},\lambda)  \s \in \s \mathbb{R}^{k+1} }{\text{argmax}}
         \s
         \sum_{j = 1}^k \left(\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1\right) \log\pi_j+ \lambda g(\bm{ \pi}) 
 \end{align*} 
 Here we ignore the normalizing constants which become an additive constants in the log 
 posterior optimization problem.
 The solution is found where 
 \begin{align*}
         \nabla_{\bm{ \pi}}  \log p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) + \lambda g(\bm{ \pi}) &= 0 \\
         g(\bm{ \pi}) &= 0
 \end{align*} 
 The first condition yields
 \[
         \left[ \nabla_{\bm{ \pi}} 
         \log p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) 
         + \lambda g(\bm{ \pi}) 
 \right]_\ell
 \bigg|_{\substack{\pi_\ell = \pi_\ell^*\\ \lambda = \lambda^*}}   = 0 
         \implies \frac{\sum_{i = 1}^n x_\ell^{(i)} + \alpha_\ell - 1}{\pi^*_\ell}
          =\lambda^*
 \]
 Replacing this result in the second condition, we get
 \[
         1 - \sum_{j = 1}^k \frac{\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1}{\lambda^*} = 0
         \implies 
         \lambda^* = n + \sum_{j = 1}^k \alpha_j - k
 \]
 Where we swapped the sum over the $x_j^{(i)}$ and used the fact that  $\mathbf{x}_j$ are 
 one hot vectors. Thus
 \[
         \boxed{(\hat{\bm{ \pi} }_{\text{MAP}} )_j = \pi^*_j = 
         \frac{\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1}{n + \sum_{j = 1}^k\alpha_j -  k}\s \in \s [0,1] }
 \]
 The maximum likelihood estimator is, on the other hand,
 \[
         (\mle{\bm{ \pi} } )_j = \frac{\sum_{j = 1}^n x_j^{(i)}}{n}
 \]
 In the regime of extremely large $k$, knowing that  $\alpha_j > 1 \s \forall \s j$, we 
 expect the sum $\sum_{j = 1}^k\alpha_j - k \gg 1$ to become non-negligible. In turns, 
 this means that we expect
  \[
          \boxed{ (\bm{ \hat{\pi}}_{\text{MAP}}  )_j < (\mle{\bm{ \pi}} )_j}
 \]



 \section{Properties of estimators}
 \subsection{Poisson}
 Let n trials $X_1,\dots,X_n \overset{\text{iid}}{\sim} \text{Poisson}(\lambda) $ where 
 $\lambda = \E_x[x]$. The 
 pmf of the Poisson is
 \[
         p(x \mid \lambda) = \frac{\lambda^x}{x!} e^{-\lambda}\s \forall \s x \in \mathbb{N}
 \]
 Such that the pmf of $n$ trials should be
  \[
         p(x_{1:n} \mid \lambda) \propto \prod_{j = 1}^n p(x_j \mid \lambda)
 \]
 \subsubsection{MLE}
Using the log likelihood, we define the MLE estimation of $\lambda$ as
 \[
         \hat{\lambda}_{\text{MLE}} = 
                 \underset{\lambda \s \in \s \mathbb{R}_{>0}}{\text{argmax} } 
                \s
        \sum_{j = 1}^n
        (x_j \log \lambda - \lambda)
\]
Which is found where
\[
        \nabla_\lambda \log p(x_{1:n} \mid \lambda ) \bigg|_{\lambda = \lambda^*} = 0
\]
That is
\[
        \nabla_\lambda \log p(x_{1:n} \mid \lambda) = \lambda^{-1} \sum_{j = 1}^n(x_j - 1)
\]
Thus
\[
        \boxed{\hat{\lambda}_{\text{MLE}} = \frac{1}{n}\sum_{j = 1}^n x_j  }
\]


\subsubsection{Bias}
The bias is defined as 
\[
        \text{Bias}(\lambda, \mle{\lambda}) \equiv \E_x[\mle{\lambda}] - \lambda
\]
The expectation value of the MLE estimator is
\begin{align*}
        \E_x[\mle{\lambda}] &= \E_x \left[    \frac{1}{n}\sum_{j=1}^n x'_j \right] \\
                             &= \frac{1}{n} \sum_{j = 1}^k \E_x[x_j] \\
                             &= \lambda
\end{align*} 
Therefore the \textbf{MLE estimator of a Poisson distribution is an unbiased estimator}.

\subsubsection{Variance}
The variance of the estimator is
\[
        \text{Var}(\mle{\lambda}) \equiv  \E_X[\mle{\lambda}^2] - \E_x^2[\mle{\lambda}]
\]
%For the Poisson distribution, 
%\begin{align*}
        %\E_x[x^2] &= \sum_{x=0}^\infty x^2\frac{\lambda^{x}}{x!}e^{-\lambda}\\ 
                  %&= e^{-\lambda} \sum_{x = 0}^\infty 
%\end{align*}
We need to evaluate the first term. To do this, we first use the Multinomial 
theorem to expand $\mle{\lambda}^2$:
 \[
         \mle{\lambda}^2 = \frac{1}{n^2} \sum_{k_1 + \dots + k_n = 2} 
         \binom{2}{k_1,\dots,k_n} x_1^{k_1}\dots x_n^{k_n}
\]
Then we use both the linearity of the expectation operator and the fact that 
the R.V. $X_1,\dots,X_n$ are independent to factorize the expectation of a "cross" 
product
 \[
         \E_x[X_iX_j] = \E_x[X_i]\E_x[X_j], \s\s\s \forall \s\s i,j\s \s \{\text{iid}\} 
\]
to get
\[
        \E_x[\mle{\lambda}^2] = \frac{1}{n^2} \sum_{k_1 + \dots + k_n = 2} 
        \binom{2}{k_{1:n}} \E_x[x_1^{k_1}]\dots\E_x[x_n^{k_n}]
\]
The sum can be separated into quadratic and linear term, s.t.
\[
        \E_x[\mle{\lambda}^2] = \frac{1}{n^2}\left( 
                n\E_x[x^2] + 2\binom{n}{2}\lambda^2
        \right) 
\]
We used the fact that $\E_x[1] = 1$ and $\E_x[x_j] = \lambda,\s \forall j$. To estimate 
the quadratic term, we can use a magic trick by adding zero inside 
the operator argument. Using its linear property 
 \[
         \E_x[x^2] = \E_x[x(x - 1) + x] = \E_x[x(x-1)] + \lambda
\]
It turns out that
\begin{align*}
        \E_x[x(x-1)] &= \sum_{x=0}^\infty x(x-1) \frac{\lambda^x}{x!}e^{-\lambda} \\
                     &= e^{-\lambda} \sum_{x=2}^\infty \frac{\lambda^x}{(x-2)!} \\
                     &= e^{-\lambda}  \sum_{x=0}^\infty \frac{\lambda^{x + 2}}{x!} \\
                     &= \lambda^2
\end{align*} 
By noticing the sum is the Taylor series of $e^\lambda$. In the end, we get
 \[
         \E_x[\mle{\lambda}^2] = \frac{1}{n^2}\left( 
         n\lambda + n^2\lambda^2 
         \right) 
\]
Where we expanded the binom coefficient $2\binom{n}{2} = n(n-1)$.
The variance is thus
\[
        \boxed{\text{Var}(\mle{\lambda}) = \frac{\lambda}{n} }
\]

\subsubsection{Consistency}
As $n \rightarrow \infty$, the estimator give an unbiased estimate of $\lambda$ with 
a variance that goes to 0. Thus, the \textbf{estimator is consistent}. 


\subsection{Bernoulli}
Let $X_1,\dots,X_n \overset{\text{iid}}{\sim} \text{Bernoulli(p)} $ and let $n > 10$.
We consider the estimator 
 \[
         \hat{p} \equiv \frac{1}{10}\sum_{i = 1}^{10} X_i 
\]


\subsubsection{Bias}
We first note that the expected value of a Bernoulli is 
\[
        \E_x[x] = p
\]
Since $x \in  \{0,1\}$ and $p$ is the probability that  $X = 1$. Therefore,
\[
        \text{Bias}(p, \hat{p}) = \frac{1}{10}\sum_{j = 1}^{10} \E_x[x_j] - p = 0
\]
$\hat{p}$ is an unbiased estimator.


\subsubsection{Variance}
The variance is 
 \begin{align*}
         \text{Var}(\hat{p}) &= \E_x[\hat{p}^2] - \E_x^2[\hat{p}] \\
                             &= \frac{1}{100}\left(10 \E_x[x^2] + 90\E_x^2[x]  \right) - p^2 \\
                             &= \frac{p}{10} + p^2 \left(\frac{90}{100} - 1\right) \\
                             &= \frac{1}{10}(p - p^2)
\end{align*} 


\subsubsection{Consistency}
\textbf{This estimator is not consistent since the variance is constant as $n\rightarrow \infty$. }


\subsection{Uniform}
Let $X_1,\dots,X_n \overset{\text{iid}}{\sim }\text{Uniform}(0,\theta) $. The pdf 
of this distribution is 
\[
        p(x_i \mid \theta) = \left\{  
        \begin{array}{rl}
                \theta^{-1},& x \in [0,\theta]\\
                0,& \text{otherwise} 
        \end{array}
\right. 
= \frac{1}{\theta} \bm{ 1}_{\{0 \le x_i \le \theta \}} 
\]
for $\theta \in \R_{>0}$. We used the indicator function 
\[
        \bm{ 1}_A(x) \equiv \left\{ 
        \begin{array}{lr}
                1,& x \in A \\
                0,& x \not\in A
        \end{array}
        \right. 
\]
%From this we can see that the probability of $x_i$ to 
%fall in the interval  $[0, c],\s c \le \theta$ is 
 %\[
         %p(x_i < c \mid \theta) = \frac{c}{\theta}
%\]



\subsubsection{MLE}
Given $n$ samples, we want an estimator of the maximum possible value of $\mathbf{X}$.
The MLE is
\[
        \mle{\theta} = \underset{\theta \s \in \s \R_{>0}}{\text{argmax}}
        \s
        \frac{1}{\theta^n}\prod_{i = 1}^n \bm{ 1}_{\set{0 \le x_i \le \theta}} 
\]
Where we used the fact that the data is iid. We can see that the product only depends 
on the boundary cases of the dataset, that is
\[
        \mle{\theta} = \underset{\theta \s \in \s \R_{>0}}{\text{argmax}}
        \s
        \frac{1}{\theta^n}  \bm{ 1}_{\set{0 \le \min \mathbf{X}}} 
        \bm{ 1}_{\set{\max \mathbf{X} \le \theta}}  
\]
One can see that $\theta$ should be as low as possible to maximize $\theta^{-n}$, yet 
not too low s.t. it make the second indicator function 0. The obvious choice is therefore 
\[
        \boxed{\mle{\theta} = \max \mathbf{X}}
\]
We show that $T(\theta) = \mle{\theta}$ is a sufficient statistic.
To show this, we user the Fisher-Neyman theorem which garantees that the 
statistic is sufficient if
the probability density can be factorized as 
$p(\mathbf{X}) = h(\mathbf{X}) g(\theta, T(\theta))$. First, we use 
the fact that the data is iid:
\[
        p(\mathbf{X}) = \prod_{i = 1}^n p(x_i)
\]
Then replacing by the Uniform distribution
\[
        p(\mathbf{X}) = \prod_{i = 1}^n \frac{1}{\theta}\bm{ 1}_{\{0 \le x_i \le \theta\}} 
\]
For the probability to be non-zero, only the boundary cases are important. That is
\[
p(\mathbf{X}) = \frac{1}{\theta^n} 
\bm{ 1}_{\{0 \le \min \mathbf{X} \}} 
\bm{ 1}_{\{\max \mathbf{X}\le \theta\}} 
= h(\mathbf{X}) \frac{1}{\theta^n} \bm{ 1}_{\{T(\theta) \le \theta\}} 
\]
We identify $h(\mathbf{X}) = \bm{ 1}_{\{0 \le \min\mathbf{X}\}} $ and the rest with the 
function $g(\theta, T(\theta))$. Thus we have shown  $T(\theta)$ is a 
sufficient statistic by the Fisher-Neyman theorem.


\subsubsection{Bias}
The bias of this estimator is 
\[
        \text{Bias}(\theta, \mle{\theta}) = \E_c[\mle{\theta}] - \theta 
\]
Where $c = \max \mathbf{X}$. To compute the expectation value of the MLE estimator, 
we must first compute the pdf with respect to $c$. We notice that the likelihood of 
the maximum in the set $\mathbf{X}$ to bge smaller than c is
\[
        p(\max \mathbf{X} < c \mid \theta) = \prod_{i = 1}^n \left( \frac{c}{n} \right) 
         = \left( \frac{c}{n} \right)^n 
\]
from the hint. To get the pdf, we derive this expression with respect to $c$ (the 
likelihood over the region $x_i \in [0, c]$ is an integral, so we expect the pdf 
to be the derivative of this integral):
 \[
         p(\max \mathbf{X} = c \mid \theta ) = n \frac{c^{n - 1}}{\theta^n}
\]
Therefore, 
\[
        \E_c[\mle{\theta}] = n\int_0^\theta c\frac{c^{n  - 1}}{\theta^n}dc
        = \frac{n}{n + 1}\theta
\]
And we get
\[
        \boxed{  \norm{\text{Bias}(\theta, \mle{\theta})} =  \frac{\theta}{n  + 1}}
\]


\subsubsection{Variance}
The variance is 
\[
\text{Var}(\mle{\theta}) = \E_c[\mle{\theta}^2] - \E_c^2[\mle{\theta}] 
\]
Thus
\[
        \boxed{\text{Var}(\mle{\theta}) = 
\theta^2 \left( \frac{n}{n + 2} - \frac{n^2}{(n + 1)^2} \right)  \right)} 
\]

\subsubsection{Consistency }
\textbf{The estimator is consistent because the bias and the variance go to zero as 
$n\rightarrow \infty$} 


\subsection{Gaussian}
Let $X_1,\dots,X_n \overset{\text{iid}}{\sim} \mathcal{N}(\mu,\sigma^2),\s  \s
\sigma, \mu \in \R $. We define the mean as $\bar{X} = \frac{1}{n}\sum_{i =1}^n x_i$.

\subsubsection{MLE}
The likelihood, using the fact that the data is iid, is
\[
        p(\mathbf{X} \mid \mu,\sigma) = \frac{1}{ (\sigma \sqrt{2\pi})^n} 
        \prod_{i = 1}^n 
        \exp\left( -\frac{1}{2}\frac{(x_i - \mu)^2}{\sigma^2} \right) 

\]
The MLE for $\mu$ and $\sigma^2$ can be derived from the log likelihood
 \[
         \mle{\theta} = (\mle{\mu}, \mle{\sigma}^2) = 
         \underset{ (\mu, \sigma^2) \s \in \s \R^2}{\text{argmax}} 
                 \s
                 -\frac{n}{2} \log \sigma^2
                 -\frac{1}{2}\sum_{i = 1}^n \frac{(x_i - \mu)^2}{\sigma^2}
\]
The solution is found where
\[
        \nabla_\theta \log p(\mathbf{X} \mid \theta) = 0
\]
That is
\[
        \partial_\mu \implies  \sum_{i = 1}^n (x_i - \hat{\mu}) = 0
\]
From which we find
\[
        \hat{\mu} = \frac{1}{n}\sum_{i = 1}^n x_i = \bar{X}
\]
Also, we have
\[
        \partial_{\sigma^2} \implies -\frac{n}{2\hat{\sigma}^2} + 
        \frac{1}{2}\sum_{i =1}^n \frac{(x_i - \bar{X})^2}{\hat{\sigma}^4} = 0
\]
Thus
\[
        \hat{\sigma}^2 = \frac{1}{n}\sum_{i = 1}^n (x_i - \bar{X})^2 
\]


\subsubsection{Bias}
The bias for the normal variance MLE $\mle{\sigma}^2$ is
 \[
         \text{Bias}(\sigma^2, \mle{\sigma}^2) = \E_x[\mle{\sigma}^2] - \sigma^2 
\]
Thus,
\begin{align*}
        \text{Bias}(\sigma^2, \mle{\sigma}^2) &= 
        \frac{1}{n}\sum_{i = 1}^n (\E_x[x_i^2] - 2 \E_x[x_i]\E_x[\bar{X}] + \E_x[\bar{X}^2])
\end{align*} 
With the definition of the variance, we can replace $\E_x[x^2] = \sigma^2 + \mu^2$.
Thus,
 \begin{align*}
         \text{Bias}(\sigma^2, \mle{\sigma}^2) &= 
        \frac{1}{n}\sum_{i = 1}^n (\sigma^2 + \mu^2 - 2 \mu^2 + \E_x[\bar{X}^2])
\end{align*} 
Expanding the square of the mean
\[
\bar{X}^2 = \frac{1}{n^2}\sum_{i = 1}^n
\left(  x_i^2 + 2\sum_{j > i}x_ix_j
\right)
\]
Thus
\[
        \E_x[\bar{X}^2] = \frac{1}{n}\sigma^2 + \frac{1}{n}\mu^2 + \frac{(n - 1)}{n}\mu^2 =
                \frac{1}{n}\sigma^2 + \mu^2
\]
Finally,
\[
        \norm{\text{Bias}(\sigma^2, \mle{\sigma}^2)} = \frac{\sigma^2}{n} 
\]
%\[
        %\E_x[\mle{\sigma}^2] = \frac{1}{n\sigma \sqrt{2\pi}}\sum_{i = 1}^n 
        %\int_{-\infty}^\infty 
        %(x_i - \bar{X})^2 \exp \left( -\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2} \right) 
        %dx
%\]
%We used the linearity of the integral to swap the sum operator. We define some 
%useful properties of the gaussian integral which we do not derive. They 
%correspond to the non-central moments of the gaussian.
%\begin{align*}
        %\frac{1}{n\sqrt{2\pi}} \exp \left( - \frac{1}{2} x^2 \right) &= \varphi(x) \\
        %\int_{-\infty}^\infty \varphi \left(  \frac{x - \mu}{\sigma } \right) dx &= 1  \\
        %\int_{-\infty}^\infty  x \varphi \left( \frac{x - \mu}{\sigma} \right) dx &= \mu \\
        %\int_{-\infty}^\infty x^2 \varphi \left( \frac{x - \mu}{\sigma^2} \right)dx &=
        %\mu^2 + \sigma^2 
%\end{align*} 
%Thus,
%\begin{align*}
        %\E_x[\mle{\sigma}^2] &= \frac{1}{n}
        %\sum_{i = 1}^n \int_{-\infty}^\infty
        %\left(  x_i^2 -
        %\frac{2}{n} x_i \sum_{j = 1}^n x_j + \frac{1}{n^2} \left( \sum_{j = 1}^n x_j \right)^2 
                %\right)
                %\varphi \left( \frac{x - \mu}{\sigma} \right) dx \\
        %&= \frac{1}{n}\sum_{i = 1}^n \int_{-\infty}^\infty 
        %\left( 
                %x^2 - 2nx^2 + nx^2 + \frac{n - 1}{n}x^2
        %\right) 
        %\varphi \left( \frac{x - \mu}{\sigma} \right) dx
%\end{align*} 


\subsubsection{Variance}
%The $\mle{\sigma}^2$ variance is 
%\[
        %\text{Var}(\mle{\sigma}^2) = \E_x[\mle{\sigma}^4] - \E_x^2[\mle{\sigma}^2] 
%\]
We can use the chi-squared distribution for which we know the variance
\[
        \text{Var}(\chi^2_{n -1}) = 2(n - 1)
\]
Knowing that
\[
        \chi^2_{n -1} \equiv \frac{1}{\sigma^2}\sum_{i = 1}^n (x_i -\bar{X})^2 =
        \frac{n\mle{\sigma}^2}{\sigma^2}
\]
%Thus, 
%\[
        %\E_x[\mle{\sigma}^4] = \text{Var}(\mle{\sigma}^2) + \E_x^2[\mle{\sigma}^2] 
%\]
Thus
\[
\text{Var}(\mle{\sigma}^2) = \text{Var}\left( \frac{\chi^2_{n -1}\sigma^2}{n} \right)   
= \frac{2\sigma^4(n - 1)}{n^2}
\]

\subsubsection{Consistency}
Both the variance and the bias of the estimator 
go to 0 as $n \rightarrow 0$, so the \textbf{estimator of 
the variance is consistent.}

\end{document}
