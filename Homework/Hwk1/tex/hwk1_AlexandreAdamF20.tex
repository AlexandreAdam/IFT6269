\documentclass{article}
\usepackage[a4paper, margin=2cm]{geometry}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{amssymb}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}

\pagestyle{fancy}
\rhead{Alexandre Adam \\ 20090755}
\lhead{IFT6269 â€• Probabilistic Graphical Models \\ Simone Lacoste-Julien}
\chead{Homework 1}
\rfoot{\today}
\cfoot{\thepage}


\newcommand{\s}{\hspace{0.1cm}}
\numberwithin{equation}{section}
\renewcommand\thesubsection{\alph{subsection})}
\renewcommand\thesubsubsection{\Roman{subsubsection}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\mle}[1]{\hat{#1}_{\text{MLE}}}
\newcommand{\set}[1]{\{#1\}}

\begin{document}

\section{Probability and independence}
\subsection{Decomposition}
We aim to validate
\begin{equation}
        (X \perp Y, W \mid  Z)  \implies (X \perp Y \mid Z)
\end{equation}
\textit{Proof}.  We suppose the statement $(X \perp Y,\ W \mid Z)$ is true. 
It follows from the definition of the conditional independence that
$p(x, y, w \mid z) = p(x\mid z) p(y, w\mid z)$ for all 
$x \in  \Omega_x$, $(y, w) \in \Omega_y \times  \Omega_w$  and $z \in  \Omega_z$.
We then consider the marginalize  $p(x,y,w \mid z)$:
\begin{align*}
        p(x,y|z) &=  \sum_{w\s \in \s \Omega_w}  p(x, y , w \mid z) \\
                 &=  \sum_{w\s \in \s \Omega_w} p(x\mid z)p(y,w \mid z) \\
                 &= p(x \mid z) \sum_{w\s  \in \s  \Omega_w} p(y,w \mid z) \\
                &=  p(x\mid z) p(y\mid  z)
\end{align*}
from which we conclude that  $(X \perp Y \mid Z) \qed$.
By symmetry of the argument,  we can show that ${(X \perp W \mid Z)}$ is true as well.

\subsection{}
We aim to validate 
\begin{equation}\label{eq:1b}
       (X \perp Y \mid Z) \s \text{and} \s (X, Y \perp  W \mid Z) \implies (X \perp W \mid Z)  
\end{equation}
\textit{Proof}. Suppose $(X, Y \perp  W \mid Z)$ and $(X \perp Y \mid Z)$ are true.
We know from the symmetry and decomposition properties of  
the  conditional independence that  
$(X, Y \perp W \mid  Z)  \implies  (W \perp X, Y \mid Z)  \implies  (X \perp  W \mid Z)$.
Therefore $ (X \perp W \mid Z)$ is true $\qed$.


\subsection{}
We aim to validate
\begin{equation}
        (X \perp Y,W \mid Z) \s \text{and} \s (Y \perp W \mid Z) \implies  (X,W \perp Y \mid Z)
\end{equation}
\textit{Proof}. Suppose $(X \perp Y,W \mid Z)$ is true. 
Then it follows from the definition of conditional independence that
\begin{align*}
        p(x,y,w \mid z) = p(x \mid z) p(y,w \mid z) 
\end{align*}
Then assume $(Y \perp W \mid Z)$ is true. The second factor can be factorized
\begin{align*}
        p(x,y,w \mid z) = p(x \mid z) p(y \mid z) p(w \mid z)
\end{align*}
From the decomposition property, we know $(X \perp W \mid Z)$ is true. Thus
\begin{align*}
        p(x,y,w \mid z)  = p(x,w \mid z) p(y \mid z)
\end{align*}
From which we conclude $(X,W \perp Y \mid Z)$ is true $\qed$.

\subsection{}
We aim to validate
\begin{equation}
        (X \perp Y \mid Z) \s \text{and} \s (X \perp Y \mid W) \implies (X \perp Y \mid Z,W)
\end{equation}
\textit{Counter example}. We consider the following R.V.
\begin{enumerate}
        \item X: Person A arrive late for diner;
        \item Y: Person B arrive late for diner;
        \item W: They come from the same city;
        \item Z: They work in the same city.
\end{enumerate}
For this situation, we see that X and Y are conditionally independent when 
given either W or Z. If we know they are from the same city, then they might work 
in different cities and take different route home. Thus knowing person A was late doesn't inform 
us on the probability of person B to arrive late. \par
A similar argument can be made for  $(X \perp Y \mid Z)$. \par
Thus the LHS of the proposition is true, yet the RHS is clearly false in our case. 
Assuming we were given that W and Z are true, then we are given the geolocalisation 
of person A and B.
If we were given that 
person A would be late for diner, then we'd be able to make a good guess that person B would 
be late as well (they would both be impacted by the same traffic jam or whatnot).
Thus the proposition is false.

\section{Bayesian inferance and MAP}
Let $\mathbf{X}_1, \dots , \mathbf{X}_n \bm{\mid} \bm{\pi} \overset{\text{iid} }{\sim} 
\text{Multinomial} (1, \bm{ \pi} )$ on k element. The values are sampled from a set of cardinality 
2, that is $x^{(i)}_j \in \left\{ 0, 1 \right\}$. Each R.V. has only one non-zero 
entry for a given trial, that is $\sum_{j = 1}^k x^{(i)}_j = 1$. \par
We assume a Dirichlet prior $\bm{ \pi} \sim \text{Dir} ( \bm{ \alpha} )$ with 
a PDF
\[
        p(\bm{ \pi}  \mid \bm{ \alpha} ) = \frac{\Gamma ( \sum _{i = 1}^k \alpha_j) }{
        \prod_{j=1}^k \Gamma(\alpha_j)} \prod_{j = 1}^k \pi_j^{\alpha_j - 1}
\]

\subsection{}
Since the data is IID, they are mutually independent by definition. Being given the 
parameters of their Multinomial distribution (or a subset for that matter) 
does not change the independence of the $\mathbf{X}$'s. Thus, 
\[
        (\mathbf{X}_i \perp \mathbf{X}_j \mid \bm{ \pi}) \s \forall \s (i,j) \s \in 
        \left\{ 1,\dots ,k \right\} \times \left\{ 1,\dots ,k \right\}
\]
Of course, none of the vector can be mutually nor conditionally independent to $\bm{ \pi} $ 
since it contains information about the distribution of the one hot vectors $\mathbf{X}_i$. 
In this case $\bm{ \pi} $ are the probabilities of one of the $k$ entry to be equal 
to one. Even giving one of these away is enough to impact the posterior distribution 
of the conditional $p(x_i\mid x_\ell, \pi_j )$ for example.

\subsection{}
The posterior distribution $p( \bm{ \pi} \mid x_1 , \dots , x_n) $ is computed via 
the Bayes rule
\[
        p(\bm{ \pi} \mid \mathbf{x}_{1:n}) = 
        \frac{p(\mathbf{x}_{1:n} \mid \bm{ \pi} ) p(\bm{ \pi}) }{p(\mathbf{x}_{1:n})}
\]
where $p(\bm{ \pi} ) = p(\bm{ \pi} \mid \bm{\alpha} )$ is the prior for $\bm{ \pi} $
defined above. For the sake of determining the 
posterior distribution, we can postpone the derivation of the marginal likelihood.
%The likelihood is the joint probability of $n$ 
The likelihood $p(\mathbf{x}_{1:n} \mid \bm{ \pi} )$ is the probability mass function 
corresponding to $n$ trials of a $k-$sided die throw. We define the vector 
$\bm{\chi} \equiv \sum_{i=1}^n \mathbf{x}_j$ with the property
\[
        \sum_{j = 1}^k \chi_j = n
\]
It becomes clear that the likelihood follows the 
$ \text{Multinomial}(n, \bm{ \pi})  $ distribution. 
The PMF is given by
\[
        p(\mathbf{x}_{1:n} \mid \bm{ \pi} ) =
\binom{n}{\chi_1,\dots,\chi_k}
        \prod_{j=1}^k \pi_j^{\chi_j}
        \propto 
        \prod_{i = 1}^n
        \prod_{j=1}^k \pi_j^{x_j^{(i)}}
\]
Where it is agreed that $\chi_j = \sum_{i = 1}^n x^{(i)}_j$. 
Therefore, the posterior must be
\[
        p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) \propto 
        \prod_{i = 1}^n \prod_{j=1}^k \pi_j^{x_j^{(i)}}
        %\left( \frac{\Gamma(\sum_{\ell = 1}^k \alpha_\ell)}{\prod_{\ell}^k \Gamma(\alpha_{\ell})}
                \prod_{\ell = 1}^k \pi_\ell^{\alpha_\ell -1}
        %\right) 
\] 
We use the fact that we can swap around product operator for real numbers.
\[
         p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) \propto 
        \prod_{i = 1}^n \prod_{j=1}^k 
                      \prod_{\ell = 1}^k \pi_\ell^{\alpha_\ell -1}\pi_j^{x_j^{(i)}}
                      = \prod_{j = 1}^k \pi_j^{\sum_{i = 1}^n x_j^{(i)} + \alpha_j -1}

\]
We can readily see that the resulting distribution will be a Dirichlet with 
updated $\alpha_\ell$'s. 

\textbf{The posterior distribution is a \underline{Dirichlet} distribution with parameters 
$\alpha_j'= \alpha_j + \sum_{i = 1}^n x_j^{(i)} $}.

\subsection{Marginal Likelihood}
 The marginal likelihood $p(\mathbf{x}_{1:n})$ 
is a normalizing constant defined as the 
integral of the numerator over all instantiation of $\bm{ \pi} $
\[
        p(\mathbf{x}_{1:n}) = \int_{\bm{ \Delta_k}} 
        p(\mathbf{x}_{1:n} \mid \bm{ \pi})  p(\bm{ \pi} )d^{(k)}\bm{ \pi}  
\]
where $\bm{ \Delta}_k $ is the probability simplex. In term of the quantities 
defined above, this is
\[
         p(\mathbf{x}_{1:n}) = \int_{\bm{ \Delta_k}} 
         d^{(k)}\bm{ \pi} 
         \binom{n}{\chi_1,\dots,\chi_k}
       \prod_{j = 1}^k \pi_j^{\chi_j}
        \left( 
         \frac{\Gamma ( \sum _{\ell = 1}^k \alpha_\ell) }{\prod_{\ell=1}^k \Gamma(\alpha_\ell)} 
         \prod_{\ell = 1}^k \pi_\ell^{\alpha_\ell - 1}
        \right) 
\]
The $\pi_j$'s are independent variables since the simplex $\bm{ \Delta}_k $ is crucially 
defined as an affine plane in an Euclidian space which is supported by 
a set of orthonormal vectors.
%The crucial point is that the base space is an Euclidian space 
%spaned by orthonormal vectors.
Thus, our task is to evaluate $k$ identical integrals of the form
\[
        \int_0^1 d\pi_j \pi_j^{\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1}
        = 
        \left( \sum_{i = 1}^n x_j^{(i)} + \alpha_j \right)^{-1},
        \s\s\s
        \left\{ \alpha_j > 0 \right\}
\]
Thus
\[
        p(\mathbf{x}_{1:n}) = 
         \binom{n}{\chi_1,\dots,\chi_k}
         \frac{\Gamma ( \sum _{\ell = 1}^k \alpha_\ell) }{\prod_{\ell=1}^k \Gamma(\alpha_\ell)} 
 \prod_{j = 1}^k\left( \sum_{i = 1}^n x_j^{(i)} + \alpha_j \right)^{-1}


\]
 \subsection{$\hat{\bm{\pi}}_{\text{MAP}}  $}
 The maximum \textit{a posteriori} of the Multinomial distribution is
 \[
         \bm{ \hat{\pi}}_{\text{MAP}} \equiv \underset{\bm{ \pi}  \s \in \s \bm{ \Delta}_k }{\text{argmax}}
         \s
         p(\bm{ \pi} \mid \mathbf{x}_{1:n}) 
 \]
 Where the probability simplex is defined as 
 \[
         \bm{\Delta }_k = \left\{ \bm{ \pi}  \in \mathbb{R}^k \Biggm| \pi_j \s\in [0,1]\s 
                 \text{ and }\s \sum_{j = 1}^k \pi_j = 1 
         \right\}
 \]
 To satisfy the constraint $g(\bm{ \pi})= 1 - \sum_{j=1}^k \pi_j $, we use the Lagrange 
 multiplier $\lambda$ s.t. the optimisation of the log posterior becomes
 \begin{align*}
         \bm{ \hat{\pi}}_{\text{MAP}} &= 
         \underset{(\bm{ \pi},\lambda)  \s \in \s \mathbb{R}^{k+1} }{\text{argmax}}
         \s
         \sum_{j = 1}^k \left(\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1\right) \log\pi_j+ \lambda g(\bm{ \pi}) 
 \end{align*} 
 Here we ignore the normalizing constants which become additive constants in the log 
 posterior optimization problem.
 The solution is found where 
 \begin{align*}
         \nabla_{\bm{ \pi}}  \log p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) + \lambda g(\bm{ \pi}) &= 0 \\
         g(\bm{ \pi}) &= 0
 \end{align*} 
 The first condition yields
 \[
         \left[ \nabla_{\bm{ \pi}} 
         \log p(\bm{ \pi} \mid \mathbf{x}_{1:n} ) 
         + \lambda g(\bm{ \pi}) 
 \right]_\ell
 \bigg|_{\substack{\pi_\ell = \pi_\ell^*\\ \lambda = \lambda^*}}   = 0 
         \implies \frac{\sum_{i = 1}^n x_\ell^{(i)} + \alpha_\ell - 1}{\pi^*_\ell}
          =\lambda^*
 \]
 Replacing this result in the second condition, we get
 \[
         1 - \sum_{j = 1}^k \frac{\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1}{\lambda^*} = 0
         \implies 
         \lambda^* = n + \sum_{j = 1}^k \alpha_j - 1
 \]
 Where we swapped the sum over the $x_j^{(i)}$ and used the fact that  $\mathbf{x}_j$ are 
 one hot vectors. Thus
 \[
         \boxed{(\hat{\bm{ \pi} }_{\text{MAP}} )_j = \pi^*_j = 
         \frac{\sum_{i = 1}^n x_j^{(i)} + \alpha_j - 1}{n + \alpha_j -  1}\s \in \s [0,1] }
 \]


 \section{Properties of estimators}
 \subsection{Poisson}
 Let n trials $X_1,\dots,X_n \overset{\text{iid}}{\sim} \text{Poisson}(\lambda) $ where 
 $\lambda = \E_x[x]$. The 
 pmf of the Poisson is
 \[
         p(x \mid \lambda) = \frac{\lambda^x}{x!} e^{-\lambda}\s \forall \s x \in \mathbb{N}
 \]
 Such that the pmf of $n$ trials should be
  \[
         p(x_{1:n} \mid \lambda) \propto \prod_{j = 1}^n p(x_j \mid \lambda)
 \]
 \subsubsection{MLE}
Using the log likelihood, we define the MLE estimation of $\lambda$ as
 \[
         \hat{\lambda}_{\text{MLE}} = 
                 \underset{\lambda \s \in \s \mathbb{R}_{>0}}{\text{argmax} } 
                \s
        \sum_{j = 1}^n
        (x_j \log \lambda - \lambda)
\]
Which is found where
\[
        \nabla_\lambda \log p(x_{1:n} \mid \lambda ) \bigg|_{\lambda = \lambda^*} = 0
\]
That is
\[
        \nabla_\lambda \log p(x_{1:n} \mid \lambda) = \lambda^{-1} \sum_{j = 1}^n(x_j - 1)
\]
Thus
\[
        \boxed{\hat{\lambda}_{\text{MLE}} = \frac{1}{n}\sum_{j = 1}^n x_j  }
\]


\subsubsection{Bias}
The bias is defined as 
\[
        \text{Bias}(\lambda, \mle{\lambda}) \equiv \E_x[\mle{\lambda}] - \lambda
\]
The expectation value of the MLE estimator is
\begin{align*}
        \E_x[\mle{\lambda}] &= \E_x \left[    \frac{1}{n}\sum_{j=1}^n x'_j \right] \\
                             &= \frac{1}{n} \sum_{j = 1}^k \E_x[x_j] \\
                             &= \lambda
\end{align*} 
Therefore the \textbf{MLE estimator of a Poisson distribution is an unbiased estimator}.

\subsubsection{Variance}
The variance of the estimator is
\[
        \text{Var}(\mle{\lambda}) \equiv  \E_X[\mle{\lambda}^2] - \E_x^2[\mle{\lambda}]
\]
%For the Poisson distribution, 
%\begin{align*}
        %\E_x[x^2] &= \sum_{x=0}^\infty x^2\frac{\lambda^{x}}{x!}e^{-\lambda}\\ 
                  %&= e^{-\lambda} \sum_{x = 0}^\infty 
%\end{align*}
We need to evaluate the first term. To do this, we first use the Multinomial 
theorem to expand $\mle{\lambda}^2$:
 \[
         \mle{\lambda}^2 = \frac{1}{n^2} \sum_{k_1 + \dots + k_n = 2} 
         \binom{2}{k_1,\dots,k_n} x_1^{k_1}\dots x_n^{k_n}
\]
Then we use both the linearity of the expectation operator and the fact that 
the R.V. $X_1,\dots,X_n$ are independent to factorize the expectation of a "cross" 
product
 \[
         \E_x[X_iX_j] = \E_x[X_i]\E_x[X_j], \s\s\s \forall \s\s i,j\s \s \{\text{iid}\} 
\]
to get
\[
        \E_x[\mle{\lambda}^2] = \frac{1}{n^2} \sum_{k_1 + \dots + k_n = 2} 
        \binom{2}{k_{1:n}} \E_x[x_1^{k_1}]\dots\E_x[x_n^{k_n}]
\]
The sum can be separated into quadratic and linear term, s.t.
\[
        \E_x[\mle{\lambda}^2] = \frac{1}{n^2}\left( 
                n\E_x[x^2] + 2\binom{n}{2}\lambda^2
        \right) 
\]
We used the fact that $\E_x[1] = 1$ and $\E_x[x_j] = \lambda,\s \forall j$. To estimate 
the quadratic term, we can use a magic trick by adding zero inside 
the operator argument. Using its linear property 
 \[
         \E_x[x^2] = \E_x[x(x - 1) + x] = \E_x[x(x-1)] + \lambda
\]
It turns out that
\begin{align*}
        \E_x[x(x-1)] &= \sum_{x=0}^\infty x(x-1) \frac{\lambda^x}{x!}e^{-\lambda} \\
                     &= e^{-\lambda} \sum_{x=2}^\infty \frac{\lambda^x}{(x-2)!} \\
                     &= e^{-\lambda}  \sum_{x=0}^\infty \frac{\lambda^{x + 2}}{x!} \\
                     &= \lambda^2
\end{align*} 
By noticing the sum is the Taylor series of $e^\lambda$. In the end, we get
 \[
         \E_x[\mle{\lambda}^2] = \frac{1}{n^2}\left( 
         n\lambda + n^2\lambda^2 
         \right) 
\]
Where we expanded the binom coefficient $2\binom{n}{2} = n(n-1)$.
The variance is thus
\[
        \boxed{\text{Var}(\mle{\lambda}) = \frac{\lambda}{n} }
\]

\subsubsection{Consistency}
As $n \rightarrow \infty$, the estimator give an unbiased estimate of $\lambda$ with 
a variance that goes to 0. Thus, the \textbf{estimator is consistent}. 


\subsection{Bernoulli}
Let $X_1,\dots,X_n \overset{\text{iid}}{\sim} \text{Bernoulli(p)} $ and let $n > 10$.
We consider the estimator 
 \[
         \hat{p} \equiv \frac{1}{10}\sum_{i = 1}^{10} X_i 
\]


\subsubsection{Bias}
We first note that the expected value of a Bernoulli is 
\[
        \E_x[x] = p
\]
Since $x \in  \{0,1\}$ and $p$ is the probability that  $X = 1$. Therefore,
\[
        \text{Bias}(p, \hat{p}) = \frac{1}{10}\sum_{j = 1}^{10} \E_x[x_j] - p = 0
\]
$\hat{p}$ is an unbiased estimator.


\subsubsection{Variance}
The variance is 
 \begin{align*}
         \text{Var}(\hat{p}) &= \E_x[\hat{p}^2] - \E_x^2[\hat{p}] \\
                             &= \frac{1}{100}\left(10 \E_x[x^2] + 90\E_x^2[x]  \right) - p^2 \\
                             &= \frac{p}{10} + p^2 \left(\frac{90}{100} - 1\right) \\
                             &= \frac{1}{10}(p - p^2)
\end{align*} 


\subsubsection{Consistency}
\textbf{This estimator is not consistent since the variance is constant as $n\rightarrow \infty$. }


\subsection{Uniform}
Let $X_1,\dots,X_n \overset{\text{iid}}{\sim }\text{Uniform}(0,\theta) $. The pdf 
of this distribution is 
\[
        p(x_i \mid \theta) = \left\{  
        \begin{array}{rl}
                \theta^{-1},& x \in [0,\theta]\\
                0,& \text{otherwise} 
        \end{array}
\right. 
= \frac{1}{\theta} \bm{ 1}_{\{0 \le x_i \le \theta \}} 
\]
for $\theta \in \R_{>0}$. We used the indicator function 
\[
        \bm{ 1}_A(x) \equiv \left\{ 
        \begin{array}{lr}
                1,& x \in A \\
                0,& x \not\in A
        \end{array}
        \right. 
\]
%From this we can see that the probability of $x_i$ to 
%fall in the interval  $[0, c],\s c \le \theta$ is 
 %\[
         %p(x_i < c \mid \theta) = \frac{c}{\theta}
%\]



\subsubsection{MLE}
Given $n$ samples, we want an estimator of the maximum possible value of $\mathbf{X}$.
The MLE is
\[
        \mle{\theta} = \underset{\theta \s \in \s \R_{>0}}{\text{argmax}}
        \s
        \frac{1}{\theta^n}\prod_{i = 1}^n \bm{ 1}_{\set{0 \le x_i \le \theta}} 
\]
Where we used the fact that the data is iid. We can see that the product only depends 
on the boundary cases of the dataset, that is
\[
        \mle{\theta} = \underset{\theta \s \in \s \R_{>0}}{\text{argmax}}
        \s
        \frac{1}{\theta^n}  \bm{ 1}_{\set{0 \le \min \mathbf{X}}} 
        \bm{ 1}_{\set{\max \mathbf{X} \le \theta}}  
\]
One can see that $\theta$ should be as low as possible to maximize $\theta^{-n}$, yet 
not too low s.t. it make the second indicator function 0. The obvious choice is therefore 
\[
        \boxed{\mle{\theta} = \max \mathbf{X}}
\]
We show that $T(\theta) = \mle{\theta}$ is a sufficient statistic.
To show this, we user the Fisher-Neyman theorem which garantees that the 
statistic is sufficient if
the probability density can be factorized as 
$p(\mathbf{X}) = h(\mathbf{X}) g(\theta, T(\theta))$. First, we use 
the fact that the data is iid:
\[
        p(\mathbf{X}) = \prod_{i = 1}^n p(x_i)
\]
Then replacing by the Uniform distribution
\[
        p(\mathbf{X}) = \prod_{i = 1}^n \frac{1}{\theta}\bm{ 1}_{\{0 \le x_i \le \theta\}} 
\]
For the probability to be non-zero, only the boundary cases are important. That is
\[
p(\mathbf{X}) = \frac{1}{\theta^n} 
\bm{ 1}_{\{0 \le \min \mathbf{X} \}} 
\bm{ 1}_{\{\max \mathbf{X}\le \theta\}} 
= h(\mathbf{X}) \frac{1}{\theta^n} \bm{ 1}_{\{T(\theta) \le \theta\}} 
\]
We identify $h(\mathbf{X}) = \bm{ 1}_{\{0 \le \min\mathbf{X}\}} $ and the rest with the 
function $g(\theta, T(\theta))$. Thus we have shown  $T(\theta)$ is a 
sufficient statistic by the Fisher-Neyman theorem.


\subsubsection{Bias}
The bias of this estimator is 
\[
        \text{Bias}(\theta, \mle{\theta}) = \E_x[\mle{\theta}] - \theta 
         = \frac{1}{\theta}\int_0^\theta \max \mathbf{X} dx - \theta
\]
Integrating $\max \mathbf{X}$ over all possible values, we get (in absolute value)
\[
        \boxed{\norm{\text{Bias}(\theta, \mle{\theta})} = \frac{\theta}{2} }
\]


\subsubsection{Variance}
The variance is 
\[
\text{Var}(\mle{\theta}) = \E_x[\mle{\theta}^2] - \E_x^2[\mle{\theta}] 
\]
Computing the integral for all the possible values for $(\max \mathbf{X})^2$, we get 
\[
        \boxed{\text{Var}(\mle{\theta}) = 
        \frac{\theta^2}{3} - \frac{\theta^2}{4} = \frac{\theta^2}{12} }
\]

\subsubsection{Consistency }
\textbf{This estimator is not consistent.}


\subsection{Gaussian}
Let $X_1,\dots,X_n \overset{\text{iid}}{\sim} \mathcal{N}(\mu,\sigma^2),\s  \s
\sigma, \mu \in \R $. We define the mean as $\bar{X} = \frac{1}{n}\sum_{i =1}^n x_i$.

\subsubsection{MLE}
The likelihood, using the fact that the data is iid, is
\[
        p(\mathbf{X} \mid \mu,\sigma) = \frac{1}{ (\sigma \sqrt{2\pi})^n} 
        \prod_{i = 1}^n 
        \exp\left( -\frac{1}{2}\frac{(x_i - \mu)^2}{\sigma^2} \right) 

\]
The MLE for $\mu$ and $\sigma^2$ can be derived from the log likelihood
 \[
         \mle{\theta} = (\mle{\mu}, \mle{\sigma}^2) = 
         \underset{ (\mu, \sigma^2) \s \in \s \R^2}{\text{argmax}} 
                 \s
                 -\frac{n}{2} \log \sigma^2
                 -\frac{1}{2}\sum_{i = 1}^n \frac{(x_i - \mu)^2}{\sigma^2}
\]
The solution is found where
\[
        \nabla_\theta \log p(\mathbf{X} \mid \theta) = 0
\]
That is
\[
        \partial_\mu \implies  \sum_{i = 1}^n (x_i - \hat{\mu}) = 0
\]
From which we find
\[
        \hat{\mu} = \frac{1}{n}\sum_{i = 1}^n x_i = \bar{X}
\]
Also, we have
\[
        \partial_{\sigma^2} \implies -\frac{n}{2\hat{\sigma}^2} + 
        \frac{1}{2}\sum_{i =1}^n \frac{(x_i - \bar{X})^2}{\hat{\sigma}^4} = 0
\]
Thus
\[
        \hat{\sigma}^2 = \frac{1}{n}\sum_{i = 1}^n (x_i - \bar{X})^2 
\]


\subsubsection{Bias}
The bias for the normal variance MLE $\mle{\sigma}^2$ is
 \[
         \text{Bias}(\sigma^2, \mle{\sigma}^2) = \E_x[\mle{\sigma}^2] - \sigma^2 
\]
Where
\[
        \E_x[\mle{\sigma}^2] = \frac{1}{n\sigma \sqrt{2\pi}}\sum_{i = 1}^n 
        \int_{-\infty}^\infty 
        (x_i - \bar{X})^2 \exp \left( -\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2} \right) 
        dx
\]
We used the linearity of the integral to swap the sum operator. We define some 
useful properties of the gaussian integral which we do not derive. They 
correspond to the non-central moments of the gaussian.
\begin{align*}
        \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{1}{2} x^2 \right) &= \varphi(x) \\
        \int_{-\infty}^\infty \varphi \left(  \frac{x - \mu}{\sigma } \right) dx &= \sigma  \\
        \int_{-\infty}^\infty  x \varphi \left( \frac{x - \mu}{\sigma} \right) dx &= \mu \sigma \\
        \int_{-\infty}^\infty x^2 \varphi \left( \frac{x - \mu}{\sigma^2} \right)dx &=
        \sigma\mu^2 + \sigma^3 
\end{align*} 
Thus,
\begin{align*}
        \E_x[\mle{\sigma}^2] &= \frac{1}{n\sigma }
        \sum_{i = 1}^n \int_{-\infty}^\infty (x_i^2 - 2x_i \bar{X} + \bar{X}^2)
                \varphi \left( \frac{x - \mu}{\sigma} \right) dx \\
                             &= \frac{1}{n\sigma} \sum_{i = 1}^n 
                             \left(\sigma \mu^2 + \sigma^3 - 2\bar{X}\mu\sigma + 
                             \bar{X}\sigma  \right)  \\
        &= \mu^2 + \sigma^2 - 2\bar{X}\mu + \bar{X}
\end{align*} 
Thus,
\[
        \boxed{\text{Bias}(\sigma, \mle{\sigma}^2) = (\mu - \bar{X})^2 }
\]


\subsubsection{Variance}
The $\mle{\sigma}^2$ variance is 
\[
        \text{Var}(\mle{\sigma}^2) = \E_x[\mle{\sigma}^4] - \E_x^2[\mle{\sigma}^2] 
\]
Using the gaussian integral properties, we can solve this system using only the operators 
$\E_x[x] = \mu$ and $\E_x[x^2] = \mu^2 + \sigma^2$.

\end{document}
